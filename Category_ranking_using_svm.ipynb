{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Category_ranking_using_svm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarvatgit/2018201009_lab4/blob/master/Category_ranking_using_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "y8gdD8J0Itgk",
        "outputId": "99fed94c-4036-45ca-d128-50429294134b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
        "data_folder = '/content/drive/My Drive/reuters21578/'\n",
        "\n",
        "sgml_number_of_files = 21\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NBeX9jmBItg6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UAjD-9uLIthE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read all categories\n",
        "category_data = []\n",
        "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0]])\n",
        "\n",
        "# Create category dataframe\n",
        "for i in category_data:\n",
        "#     print(i[1])\n",
        "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
        "news_categories = pd.DataFrame(data=category_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IHTGtY-5IthR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "15bc5c14-c1ef-420d-dd36-8ed7cd381e0e"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IAnDONjaIths",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def cleanUpSentence(r):#, stop_words = None#\n",
        "    r = r.lower().replace(\"<br />\", \" \")\n",
        "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
        "    r = BAD_SYMBOLS_RE.sub('', r)\n",
        "\n",
        "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
        "\n",
        "    words = word_tokenize(r)\n",
        "\n",
        "    for w in words:\n",
        "        w = lemmatizer.lemmatize(w)\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jaN3gc0HIth0",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse SGML files\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "  \n",
        "def makeDict(filename, document_X, document_Y):\n",
        "    with open(filename, 'rb') as file:\n",
        "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
        "\n",
        "    for newsline in content('reuters'):\n",
        "        document_categories = []\n",
        "        document_id = newsline['newid']\n",
        "        document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "        if document_body == 'None':\n",
        "            continue\n",
        "        doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
        "        doc_categories = unescape(doc_categories)\n",
        "\n",
        "        document_body = unescape(document_body)\n",
        "        topics = newsline.topics.contents\n",
        "        places = newsline.places.contents\n",
        "        people = newsline.people.contents\n",
        "        orgs = newsline.orgs.contents\n",
        "        exchanges = newsline.exchanges.contents\n",
        "        for topic in topics:\n",
        "            document_categories.append('to_' + strip_tags(str(topic)))\n",
        "\n",
        "        for place in places:\n",
        "            document_categories.append('pl_' + strip_tags(str(place)))\n",
        "        for person in people:\n",
        "            document_categories.append('pe_' + strip_tags(str(person)))\n",
        "        for org in orgs:\n",
        "            document_categories.append('or_' + strip_tags(str(org)))\n",
        "        for exchange in exchanges:\n",
        "            document_categories.append('ex_' + strip_tags(str(exchange)))\n",
        "        document_X[document_id] = document_body\n",
        "        document_Y[document_id] = document_categories\n",
        "\n",
        "def readFiles(test_data = False):\n",
        "    document_X = {}\n",
        "    document_Y = {}\n",
        "    if test_data == True:\n",
        "        file_name = sgml_file_name_template.replace('NNN', '021')\n",
        "        filename = data_folder + file_name\n",
        "        makeDict(filename, document_X, document_Y)\n",
        "    else:\n",
        "        for i in range(sgml_number_of_files):\n",
        "            if i < 10:\n",
        "                seq = '00' + str(i)\n",
        "            else:\n",
        "                seq = '0' + str(i)\n",
        "        file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "        print('Reading file: %s' % file_name)\n",
        "        filename = data_folder + file_name\n",
        "        makeDict(filename, document_X, document_Y)\n",
        "    return document_X, document_Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uqD07TIJQKy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cc95f7e-f4e5-43ee-cb6b-1321b42cfa98"
      },
      "cell_type": "code",
      "source": [
        "document_X, document_Y = readFiles()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-020.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qphgSScOIti3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5e0405ee-1aa4-4d77-e8cf-426501b188fc"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "def create_x_matrix(document_X):\n",
        "    totalX = []\n",
        "    for i, doc in document_X.items():\n",
        "        totalX.append(cleanUpSentence(doc))\n",
        "    max_vocab_size = 200\n",
        "    input_tokenizer = Tokenizer(200)\n",
        "    input_tokenizer.fit_on_texts(totalX)\n",
        "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
        "    return totalX,encoded_docs"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rKx4vpmHItjE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalX,encoded_docs=create_x_matrix(document_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zWXPxNedJKBj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "words_in_body={}\n",
        "\n",
        "for i in range(len(totalX)):\n",
        "    words=totalX[i].split(' ')\n",
        "    words_in_body[i]=words    \n",
        "\n",
        "one_hot_label=[]\n",
        "for key,v in words_in_body.items():\n",
        "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
        "    for i in v:\n",
        "        if i in category_dictionary['Topics']:\n",
        "            dict_temp['Topics']+=1\n",
        "        if i in category_dictionary['Places']:\n",
        "            dict_temp['Places']+=1\n",
        "        if i in category_dictionary['People']:\n",
        "            dict_temp['People']+=1\n",
        "        if i in category_dictionary['Exchanges']:\n",
        "            dict_temp['Exchanges']+=1\n",
        "        if i in category_dictionary['Organizations']:\n",
        "            dict_temp['Organizations']+=1\n",
        "            \n",
        "    one_hot_label.append(dict_temp)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z1qFUsPkItiD",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84179218-4389-4958-d405-e7ed7e3f1ea0"
      },
      "cell_type": "code",
      "source": [
        "#print(one_hot_label)\n",
        "import operator\n",
        "true_ranks=[]\n",
        "for i in one_hot_label:\n",
        "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
        "print(true_ranks[0])\n",
        "#print(totalX[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Topics', 7), ('People', 3), ('Places', 1), ('Exchanges', 0), ('Organizations', 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M_GQHfFTJZpR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7YIsEmSPJusK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f4vTbKfVJ1kw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer_svm = TfidfVectorizer(stop_words='english')\n",
        "mlb = MultiLabelBinarizer()\n",
        "svmlabeltrain=[]\n",
        "svmlabeltest=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DJxIV-_UJ4Cz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa41289c-3534-4b30-93e2-de55fffaa314"
      },
      "cell_type": "code",
      "source": [
        "for i in true_ranks[0:500]:\n",
        "    svm=[]\n",
        "    for j in i:\n",
        "        if j[1]!=0:\n",
        "            svm.append(j[0])\n",
        "    svmlabeltrain.append(svm)\n",
        "print(svmlabeltrain[0])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Topics', 'People', 'Places']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W81vWMuEJ6ig",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in true_ranks[500:700]:\n",
        "    svm=[]\n",
        "    for j in i:\n",
        "        if j[1]!=0:\n",
        "            svm.append(j[0])\n",
        "    svmlabeltest.append(svm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JThWRX8fJ-3L",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "#print(\"svmlabeltrain: \",type(svmlabeltrain),len(svmlabeltrain))\n",
        "Y = mlb.fit_transform(svmlabeltrain)\n",
        "\n",
        "Y=np.array(Y)\n",
        "X=np.array(totalX[0:500])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jATuWY_3KBfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e861478c-f45a-41f0-e443-6c731cc508cf"
      },
      "cell_type": "code",
      "source": [
        "classifier = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
        "#print(Y[0])\n",
        "classifier.fit(X, Y)\n",
        "print(\"X:\",X.shape)\n",
        "X_test=np.array(totalX[500:700])\n",
        "print(totalX[600])\n",
        "\n",
        "predicted = classifier.predict(X_test)\n",
        "# print(\"X_test:\",X_test.shape)\n",
        "print(\"predicted: \",predicted.shape, type(predicted))\n",
        "#print(predicted)\n",
        "print(true_ranks[0])\n",
        "#print(totalX[500])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (500,)\n",
            "shr 83 cts vs 67 cts net 10 052 000 vs 7 929 000 avg shrs 12 161 000 vs 11 824 000 nine mths shr 233 dlrs vs 191 dlrs net 28 110 000 vs 22 386 000 avg shrs 12 078 000 vs 11 729 000 assets 59 billion vs 57 billion deposits 53 billion vs 48 billion loans leases 42 billion vs 40 billion reuter\n",
            "predicted:  (200, 5) <class 'numpy.ndarray'>\n",
            "[('Topics', 7), ('People', 3), ('Places', 1), ('Exchanges', 0), ('Organizations', 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ANupfo5BoKZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "1ed984b7-f732-4ef8-836d-e3e2bd3fb6be"
      },
      "cell_type": "code",
      "source": [
        "# from sklearn.multiclass import OneVsRestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "# tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,2), token_pattern='(\\S+)')\n",
        "# X_train = tfidf_vectorizer.fit_transform(totalX[0:500])\n",
        "#    # X_val = tfidf_vectorizer.transform(X_val)\n",
        "# X_test = tfidf_vectorizer.transform(totalX[500:700])\n",
        "# y_train = mlb.fit_transform(svmlabeltrain)\n",
        "# clf = LogisticRegression(C=5,solver='liblinear')\n",
        "#     #clf = OneVsRestClassifier(clf)\n",
        "# clf.fit(X_train, y_train)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-0f7ac94d3ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#clf = OneVsRestClassifier(clf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (500, 5)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7mW11acezHy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QPYTWp7DItjX",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d09a4e0-07a0-48b0-a77c-50e47f9588cb"
      },
      "cell_type": "code",
      "source": [
        "# print(predicted)\n",
        "all_labels = mlb.inverse_transform(predicted)\n",
        "print(all_labels[0])\n",
        "# print(len(svmlabeltest))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}